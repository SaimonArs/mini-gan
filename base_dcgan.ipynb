{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9850e97-90ca-4464-9a40-9abf0268a2a6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a14211-5dfd-4356-a18e-79224476870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bf22f96-70b1-4a3a-8bea-0dcec20ed36a",
   "metadata": {},
   "source": [
    "l1 = nn.ConvTranspose2d(128, 64, kernel_size=7, stride=1, padding=0)\n",
    "l2 = nn.Conv2d(1, 128, kernel_size=3, stride=1, padding=1)\n",
    "l3 = nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1)\n",
    "X = torch.rand(size=(128, 1, 1))\n",
    "l3(l1(X)).shape\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "929274aa-0c30-4577-9200-eaba1f306495",
   "metadata": {},
   "source": [
    "l1 = nn.ConvTranspose2d(128, 256, kernel_size=7, stride=1, padding=0, bias=False)\n",
    "            \n",
    "l2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "\n",
    "l3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            \n",
    "l4 = nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "X = torch.rand(size=(128, 1, 1))\n",
    "l4(l3(l2(l1(X)))).shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c99cb136-aef8-4e0f-bd3b-6823ba8eae82",
   "metadata": {},
   "source": [
    "l1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "l2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            \n",
    "l3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            \n",
    "l4 = nn.Conv2d(256, 1, kernel_size=7, stride=1, padding=0, bias=False)\n",
    "X = torch.rand(size=(1, 28, 28))\n",
    "\n",
    "l4(l3(l2(l1(X)))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93d4037-a696-40c2-a44a-432ae1debf76",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b26c113-4e4d-4cc7-92d2-22c25f00c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANT = \"A\" # Вариант - модели (для разделения моделей с разными гиперпараметрами) result/VARIANT+SEED/\n",
    "SEED = 451 # Для получения предсказуемых результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a9a486-09eb-4d6c-a7f8-683ca41d329a",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a8d269-c583-491c-ae45-976b635f5307",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"base_dcgan\"\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "torch.set_default_device(DEVICE)\n",
    "\n",
    "LATENT_DIM = 100\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE_G = 0.0002\n",
    "LEARNING_RATE_D = 0.0002\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999\n",
    "DROPOUT_P = 0.2\n",
    "\n",
    "# Пути для сохранения\n",
    "SAVE_DIR = \"./result\"\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "RESULT_DIR = f\"{SAVE_DIR}/{MODEL_NAME}/{VARIANT}{SEED}\"\n",
    "GIF_DIR = f\"{RESULT_DIR}/gif\"\n",
    "\n",
    "Path(RESULT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(GIF_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832e951a-343a-4f82-82e4-24a4e9a87645",
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(DEVICE) == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e18927-eb90-4d99-98e7-b6111a2d1497",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fc6b8ea-59d5-4998-a9a0-604ff5a0cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(msg, log=f\"{RESULT_DIR}/hist.log\"):\n",
    "    with open(log, \"a\") as f:\n",
    "        f.write(msg + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216e6ffa-4bc6-4848-9ad7-5c18d9594205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models():\n",
    "     \"\"\"Сохранение моделей\"\"\"\n",
    "     torch.save(generator.state_dict(),\n",
    "               f\"{RESULT_DIR}/generator.pth\")\n",
    "     torch.save(discriminator.state_dict(),\n",
    "               f\"{RESULT_DIR}/discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90373d4a-20f7-457b-9d86-3d070e0aa1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gan_losses(generator_losses, discriminator_losses, model_name=\"GAN\", save_path=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(generator_losses, label='Generator loss', color='red')\n",
    "    plt.plot(discriminator_losses, label='Discriminator loss', color='blue')\n",
    "    \n",
    "    plt.title(f'{model_name} losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}/losses\", bbox_inches='tight')\n",
    "        print(f\"Save path: {save_path}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6040a96-892c-43c3-b8e6-ad8d17ec5318",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c94fe06-f7d7-4376-9189-8d6d68bcdffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"device: {DEVICE}\")\n",
    "\n",
    "write_log(\"=\"*16)\n",
    "write_log(f\"device: {DEVICE}\")\n",
    "write_log(f\"latent_dim: {LATENT_DIM}\")\n",
    "write_log(f\"epochs: {EPOCHS}\")\n",
    "write_log(f\"learning_rate_G: {LEARNING_RATE_G}\")\n",
    "write_log(f\"learning_rate_D: {LEARNING_RATE_D}\")\n",
    "write_log(f\"dropout_p: {DROPOUT_P}\")\n",
    "write_log(f\"betas: ({BETA_1},{BETA_2})\")\n",
    "write_log(f\"seed: {SEED}\")\n",
    "write_log(\"=\"*16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d273a0be-e224-4a0d-9560-a2585f4ceea5",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c61197d-7557-4f10-bf44-20f0d7e5a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dataloader(batch_size=None, data_dir='./data'):\n",
    "    \"\"\"Загрузка и подготовка датасета MNIST\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=data_dir,\n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        generator=torch.Generator(device=DEVICE),\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "976c527d-c52a-42ca-9780-2f7141d313f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_mnist_dataloader(batch_size=BATCH_SIZE, data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392789db-f8f2-4579-a89d-bc0c7571427f",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59124ea8-369a-438a-a529-426af554a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 32\n",
    "class BasicGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(BasicGenerator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(latent_dim, n*4, kernel_size=7, stride=1, padding=0, bias=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(n*4, n*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(n*2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(n*2, n*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(n*2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(n*2, n, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(n, n, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(n),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(n, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class BasicDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, n, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.Dropout2d(DROPOUT_P),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(n, n*2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(n*2),\n",
    "            nn.Dropout2d(DROPOUT_P),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(n*2, n*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(n*2),\n",
    "            nn.Dropout2d(DROPOUT_P),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(n*2, n*4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(n*4),\n",
    "            nn.Dropout2d(DROPOUT_P),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "             nn.Conv2d(n*4, n*4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(n*4),\n",
    "            nn.Dropout2d(DROPOUT_P),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Conv2d(n*4, 1, kernel_size=7, stride=1, padding=0),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d288d520-5a60-4e4e-b202-038246e12575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BasicGenerator                           [1, 1, 28, 28]            --\n",
       "├─Sequential: 1-1                        [1, 1, 28, 28]            --\n",
       "│    └─ConvTranspose2d: 2-1              [1, 128, 7, 7]            627,200\n",
       "│    └─ReLU: 2-2                         [1, 128, 7, 7]            --\n",
       "│    └─ConvTranspose2d: 2-3              [1, 64, 14, 14]           131,072\n",
       "│    └─BatchNorm2d: 2-4                  [1, 64, 14, 14]           128\n",
       "│    └─ReLU: 2-5                         [1, 64, 14, 14]           --\n",
       "│    └─ConvTranspose2d: 2-6              [1, 64, 14, 14]           36,864\n",
       "│    └─BatchNorm2d: 2-7                  [1, 64, 14, 14]           128\n",
       "│    └─ReLU: 2-8                         [1, 64, 14, 14]           --\n",
       "│    └─ConvTranspose2d: 2-9              [1, 32, 28, 28]           32,768\n",
       "│    └─BatchNorm2d: 2-10                 [1, 32, 28, 28]           64\n",
       "│    └─ReLU: 2-11                        [1, 32, 28, 28]           --\n",
       "│    └─ConvTranspose2d: 2-12             [1, 32, 28, 28]           9,216\n",
       "│    └─BatchNorm2d: 2-13                 [1, 32, 28, 28]           64\n",
       "│    └─ReLU: 2-14                        [1, 32, 28, 28]           --\n",
       "│    └─ConvTranspose2d: 2-15             [1, 1, 28, 28]            289\n",
       "│    └─Tanh: 2-16                        [1, 1, 28, 28]            --\n",
       "==========================================================================================\n",
       "Total params: 837,793\n",
       "Trainable params: 837,793\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 96.79\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 1.26\n",
       "Params size (MB): 3.35\n",
       "Estimated Total Size (MB): 4.61\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = BasicGenerator(latent_dim=LATENT_DIM)\n",
    "sm_g = summary(generator, input_size=(1, LATENT_DIM, 1, 1), device=DEVICE)\n",
    "sm_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d2bfbf0-b117-44c3-a8b8-f15e30354f02",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BasicDiscriminator                       [1, 128, 1, 1]            --\n",
       "├─Sequential: 1-1                        [1, 128, 1, 1]            --\n",
       "│    └─Conv2d: 2-1                       [1, 32, 28, 28]           288\n",
       "│    └─Dropout2d: 2-2                    [1, 32, 28, 28]           --\n",
       "│    └─LeakyReLU: 2-3                    [1, 32, 28, 28]           --\n",
       "│    └─Conv2d: 2-4                       [1, 64, 14, 14]           32,768\n",
       "│    └─BatchNorm2d: 2-5                  [1, 64, 14, 14]           128\n",
       "│    └─Dropout2d: 2-6                    [1, 64, 14, 14]           --\n",
       "│    └─LeakyReLU: 2-7                    [1, 64, 14, 14]           --\n",
       "│    └─Conv2d: 2-8                       [1, 64, 14, 14]           36,864\n",
       "│    └─BatchNorm2d: 2-9                  [1, 64, 14, 14]           128\n",
       "│    └─Dropout2d: 2-10                   [1, 64, 14, 14]           --\n",
       "│    └─LeakyReLU: 2-11                   [1, 64, 14, 14]           --\n",
       "│    └─Conv2d: 2-12                      [1, 128, 7, 7]            131,072\n",
       "│    └─BatchNorm2d: 2-13                 [1, 128, 7, 7]            256\n",
       "│    └─Dropout2d: 2-14                   [1, 128, 7, 7]            --\n",
       "│    └─LeakyReLU: 2-15                   [1, 128, 7, 7]            --\n",
       "│    └─Conv2d: 2-16                      [1, 128, 7, 7]            147,456\n",
       "│    └─BatchNorm2d: 2-17                 [1, 128, 7, 7]            256\n",
       "│    └─Dropout2d: 2-18                   [1, 128, 7, 7]            --\n",
       "│    └─LeakyReLU: 2-19                   [1, 128, 7, 7]            --\n",
       "│    └─Conv2d: 2-20                      [1, 128, 1, 1]            802,944\n",
       "==========================================================================================\n",
       "Total params: 1,152,160\n",
       "Trainable params: 1,152,160\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 28.33\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.80\n",
       "Params size (MB): 4.61\n",
       "Estimated Total Size (MB): 5.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = BasicDiscriminator()\n",
    "sm_d = summary(discriminator, input_size=(1, 1, 28, 28), device=DEVICE)\n",
    "sm_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "975e2f28-67a3-4e78-a065-3362bd37a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_log(str(sm_g))\n",
    "write_log(str(sm_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7878a8c4-56ee-4d8e-99f2-bf24b2cd3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizers\n",
    "g_optimizer = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=LEARNING_RATE_G,\n",
    "    betas=(BETA_1, BETA_2)\n",
    ")\n",
    "\n",
    "d_optimizer = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=LEARNING_RATE_D,\n",
    "    betas=(BETA_1, BETA_2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46533b05-6919-4656-82d1-f6e977d86864",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d92a104-c938-45ec-9c7f-f98dd81957d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee1678ae-bdb1-4a55-8aef-529ad195507b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def d_step(real_images, batch_size):\n",
    "    noise = torch.randn(batch_size, LATENT_DIM, 1, 1)\n",
    "\n",
    "    fake_label = torch.zeros(batch_size, 1, 1, 1)\n",
    "    real_label = torch.full((batch_size, 1, 1, 1), 0.9).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_images = generator(noise)\n",
    "\n",
    "    discriminator.train()\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    fake_pred = discriminator(fake_images.detach())\n",
    "    d_loss_fake = criterion(fake_pred, fake_label)\n",
    "    \n",
    "    real_pred = discriminator(real_images)\n",
    "    d_loss_real = criterion(real_pred, real_label) \n",
    "   \n",
    "    d_loss =  d_loss_fake + d_loss_real \n",
    "    \n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "\n",
    "    return d_loss.item()\n",
    "\n",
    "def g_step(batch_size):\n",
    "    noise = torch.randn(batch_size, LATENT_DIM, 1, 1)\n",
    "    \n",
    "    real_label = torch.ones(batch_size, 1, 1, 1)\n",
    "\n",
    "    generator.train()\n",
    "    g_optimizer.zero_grad()\n",
    "\n",
    "    fake_images = generator(noise)\n",
    "    fake_pred = discriminator(fake_images)\n",
    "\n",
    "    g_loss = criterion(fake_pred, real_label)\n",
    "\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    return g_loss.item()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033359a-88e7-4443-b5b6-996652d03b93",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9adbf4b-266e-495e-be20-02d4f894a269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63273075796b4018b9b6f8b3d02fbe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e3359be3294b5792e454a31278eea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/234 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([256, 1, 1, 1])) must be the same as input size (torch.Size([256, 128, 1, 1]))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m real_images = real_images.to(DEVICE)\n\u001b[32m     22\u001b[39m batch_size = real_images.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m epoch_d_loss += \u001b[43md_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m epoch_g_loss += g_step(batch_size)\n\u001b[32m     25\u001b[39m num_batches += \u001b[32m1\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36md_step\u001b[39m\u001b[34m(real_images, batch_size)\u001b[39m\n\u001b[32m     11\u001b[39m d_optimizer.zero_grad()\n\u001b[32m     13\u001b[39m fake_pred = discriminator(fake_images.detach())\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m d_loss_fake = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m real_pred = discriminator(real_images)\n\u001b[32m     17\u001b[39m d_loss_real = criterion(real_pred, real_label) \n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mini-GAN/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mini-GAN/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mini-GAN/lib/python3.13/site-packages/torch/nn/modules/loss.py:850\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m    849\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mini-GAN/lib/python3.13/site-packages/torch/nn/functional.py:3572\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3538\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Compute Binary Cross Entropy between target and input logits.\u001b[39;00m\n\u001b[32m   3539\u001b[39m \n\u001b[32m   3540\u001b[39m \u001b[33;03mSee :class:`~torch.nn.BCEWithLogitsLoss` for details.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3569\u001b[39m \u001b[33;03m     >>> loss.backward()\u001b[39;00m\n\u001b[32m   3570\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight):\n\u001b[32m-> \u001b[39m\u001b[32m3572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3574\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3575\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3576\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3578\u001b[39m \u001b[43m        \u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3582\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3584\u001b[39m     reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mini-GAN/lib/python3.13/site-packages/torch/overrides.py:1728\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1724\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[32m   1725\u001b[39m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[32m   1726\u001b[39m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[32m   1727\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[32m-> \u001b[39m\u001b[32m1728\u001b[39m         result = \u001b[43mmode\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1730\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mini-GAN/lib/python3.13/site-packages/torch/utils/_device.py:103\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/mini-GAN/lib/python3.13/site-packages/torch/nn/functional.py:3589\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3586\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n\u001b[32m-> \u001b[39m\u001b[32m3589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3590\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3591\u001b[39m     )\n\u001b[32m   3593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.binary_cross_entropy_with_logits(\n\u001b[32m   3594\u001b[39m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[32m   3595\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Target size (torch.Size([256, 1, 1, 1])) must be the same as input size (torch.Size([256, 128, 1, 1]))"
     ]
    }
   ],
   "source": [
    "history_losses_d = []\n",
    "history_losses_g = []\n",
    "\n",
    "#\n",
    "fixed_noise = torch.randn(64, LATENT_DIM, 1, 1).to(DEVICE)\n",
    "#\n",
    "\n",
    "for epoch in trange(EPOCHS, unit=\"epoch\"):\n",
    "    num_batches = 0\n",
    "    epoch_d_loss = 0\n",
    "    epoch_g_loss = 0\n",
    "\n",
    "    \n",
    "    tq = tqdm(\n",
    "        enumerate(dataloader),\n",
    "        total=len(dataloader),\n",
    "        leave=False,\n",
    "        unit=\"batch\",)\n",
    "    \n",
    "    for i, (real_images, _) in tq :\n",
    "        real_images = real_images.to(DEVICE)\n",
    "        batch_size = real_images.size(0)\n",
    "        epoch_d_loss += d_step(real_images, batch_size)\n",
    "        epoch_g_loss += g_step(batch_size)\n",
    "        num_batches += 1\n",
    "\n",
    "    history_losses_d.append(epoch_d_loss / num_batches)\n",
    "    history_losses_g.append(epoch_g_loss / (num_batches))\n",
    "    \n",
    "\n",
    "    info = f'epoch [{(epoch+1):>3}/{EPOCHS}], ' + \\\n",
    "         f'g_loss: {history_losses_g[-1]:.5f}, ' + \\\n",
    "         f'd_loss: {history_losses_d[-1]:.5f}'\n",
    "\n",
    "    print(info)\n",
    "\n",
    "    write_log(info)\n",
    "    generator.eval() \n",
    "    with torch.no_grad():\n",
    "        gen = generator(fixed_noise)\n",
    "        save_image(gen.view(gen.size(0), 1, 28, 28),\n",
    "                  f\"{GIF_DIR}/{epoch+1}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40ad1b-a44f-43d3-86ae-9ae34408f4e9",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd016b2b-30f6-4ccf-83ed-413ce74a9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b7e90-3517-47bb-8570-0c666611a3b1",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be24521-7a22-4347-b42b-e9441c02450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gan_losses(history_losses_g, history_losses_d, MODEL_NAME, RESULT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
